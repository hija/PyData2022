{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask.distributed import Client\n",
    "from dask_ml.wrappers import ParallelPostFit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset from git\n",
    "df = dd.read_csv('data/taiwanese-bankruptcy.csv', dtype={' Research and development expense rate': 'float64',\n",
    "       ' Total Asset Growth Rate': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_pipeline(df: dd):\n",
    "    return df.copy()\n",
    "\n",
    "def rename_columns(df: dd):\n",
    "    columns = df.columns.to_list()\n",
    "    columns_without_spaces = [column.strip() for column in columns]\n",
    "    return df.rename(columns=dict(zip(columns, columns_without_spaces)))\n",
    "\n",
    "\n",
    "cleaned_dataset = (df.pipe(start_pipeline)\n",
    "                    .pipe(rename_columns))\n",
    "\n",
    "# Because we cleaned it now, we can persist the result, so we do not need to recompute it all the time\n",
    "# It's interesting to see, how the CPU usage differs if this command is not called and the other cells are called subsequently\n",
    "cleaned_dataset = cleaned_dataset.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_dataset.drop('Bankrupt?', axis=1)\n",
    "y = cleaned_dataset['Bankrupt?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a dask function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize Training\n",
    "dt = ParallelPostFit(DecisionTreeClassifier())\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the client\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70d1c9da930b78b797d40bca365a42004d55f6fa36c41a06f8c1dc1a2deabfe7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
